"""
ç»Ÿä¸€æ•°æ®é‡‡é›†æœåŠ¡
"""
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any
from pathlib import Path
import logging
from time import sleep
from concurrent.futures import ThreadPoolExecutor, as_completed

from sqlalchemy import desc
from sqlalchemy.orm import Session

from collector.rss_collector import RSSCollector
from collector.api_collector import ArXivCollector, HuggingFaceCollector, PapersWithCodeCollector
from database import get_db
from database.models import Article, CollectionLog, RSSSource
from analyzer.ai_analyzer import AIAnalyzer

logger = logging.getLogger(__name__)


class CollectionService:
    """ç»Ÿä¸€æ•°æ®é‡‡é›†æœåŠ¡"""

    def __init__(self, config_path: str = "config/sources.json", ai_analyzer: AIAnalyzer = None):
        self.ai_analyzer = ai_analyzer
        self.config = self._load_config(config_path)

        # åˆå§‹åŒ–å„ä¸ªé‡‡é›†å™¨
        self.rss_collector = RSSCollector()
        self.arxiv_collector = ArXivCollector()
        self.hf_collector = HuggingFaceCollector()
        self.pwc_collector = PapersWithCodeCollector()

    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return {"rss_sources": [], "api_sources": [], "web_sources": [], "social_sources": []}

    def collect_all(self, enable_ai_analysis: bool = True, task_id: int = None) -> Dict[str, Any]:
        """
        é‡‡é›†æ‰€æœ‰é…ç½®çš„æ•°æ®æº

        Args:
            enable_ai_analysis: æ˜¯å¦å¯ç”¨AIåˆ†æž
            task_id: ä»»åŠ¡IDï¼Œç”¨äºŽå®žæ—¶æ›´æ–°ä»»åŠ¡çŠ¶æ€

        Returns:
            é‡‡é›†ç»Ÿè®¡ä¿¡æ¯
        """
        logger.info("ðŸš€ å¼€å§‹é‡‡é›†æ‰€æœ‰æ•°æ®æº")
        stats = {
            "total_articles": 0,
            "new_articles": 0,
            "sources_success": 0,
            "sources_error": 0,
            "start_time": datetime.now(),
        }

        db = get_db()

        # 1. é‡‡é›†RSSæºï¼ˆåŒå±‚å¹¶å‘ï¼šå¤šä¸ªRSSæº + æ¯ä¸ªæºå†…éƒ¨å¹¶å‘èŽ·å–å†…å®¹+AIåˆ†æžï¼‰
        logger.info("\nðŸ“¡ é‡‡é›†RSSæºï¼ˆåŒå±‚å¹¶å‘æ¨¡å¼ï¼‰")
        rss_stats = self._collect_rss_sources(db, task_id=task_id, enable_ai_analysis=enable_ai_analysis)
        stats.update(rss_stats)

        # å®žæ—¶æ›´æ–°ä»»åŠ¡çŠ¶æ€
        if task_id:
            self._update_task_progress(db, task_id, stats)

        # 2. é‡‡é›†APIæºï¼ˆarXiv, Hugging Faceç­‰ï¼‰
        logger.info("\nðŸ“š é‡‡é›†è®ºæ–‡APIæº")
        api_stats = self._collect_api_sources(db, task_id=task_id)
        stats.update(api_stats)

        # å®žæ—¶æ›´æ–°ä»»åŠ¡çŠ¶æ€
        if task_id:
            self._update_task_progress(db, task_id, stats)

        stats["end_time"] = datetime.now()
        stats["duration"] = (stats["end_time"] - stats["start_time"]).total_seconds()

        logger.info(f"\nâœ… é‡‡é›†å®Œæˆï¼")
        logger.info(f"   æ€»æ–‡ç« æ•°: {stats['total_articles']}")
        logger.info(f"   æ–°å¢žæ–‡ç« : {stats['new_articles']}")
        logger.info(f"   æˆåŠŸæºæ•°: {stats['sources_success']}")
        logger.info(f"   AIåˆ†æžæ•°: {stats.get('ai_analyzed_count', 0)}")
        logger.info(f"   è€—æ—¶: {stats['duration']:.2f}ç§’")

        return stats

    def _fetch_articles_full_content(self, articles: List[Dict[str, Any]], source_name: str, max_workers: int = 3) -> List[Dict[str, Any]]:
        """
        å¹¶å‘èŽ·å–æ–‡ç« çš„å®Œæ•´å†…å®¹
        
        Args:
            articles: æ–‡ç« åˆ—è¡¨
            source_name: æºåç§°
            max_workers: æœ€å¤§å¹¶å‘æ•°ï¼Œé»˜è®¤3ï¼ˆé¿å…å¯¹å•ä¸ªç½‘ç«™åŽ‹åŠ›è¿‡å¤§ï¼‰
        
        Returns:
            æ›´æ–°åŽçš„æ–‡ç« åˆ—è¡¨
        """
        # ç­›é€‰éœ€è¦èŽ·å–å®Œæ•´å†…å®¹çš„æ–‡ç« ï¼ˆblogæ–‡ç« ï¼‰
        articles_to_fetch = [
            article for article in articles 
            if article.get("category") == "rss" and article.get("url")
        ]
        
        if not articles_to_fetch:
            return articles
        
        logger.info(f"  ðŸ“„ å¼€å§‹å¹¶å‘èŽ·å– {len(articles_to_fetch)} ç¯‡æ–‡ç« çš„å®Œæ•´å†…å®¹ï¼ˆæœ€å¤§å¹¶å‘æ•°: {max_workers}ï¼‰")
        
        # å¹¶å‘èŽ·å–å®Œæ•´å†…å®¹
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_article = {
                executor.submit(self.rss_collector.fetch_full_content, article["url"]): article
                for article in articles_to_fetch
            }
            
            # æ”¶é›†ç»“æžœ
            completed = 0
            for future in as_completed(future_to_article):
                article = future_to_article[future]
                completed += 1
                
                try:
                    full_content = future.result()
                    if full_content:
                        article["content"] = full_content
                        logger.info(f"  âœ… [{completed}/{len(articles_to_fetch)}] å·²èŽ·å–å®Œæ•´å†…å®¹: {article['title'][:50]}...")
                    else:
                        logger.warning(f"  âš ï¸  [{completed}/{len(articles_to_fetch)}] æ— æ³•èŽ·å–å®Œæ•´å†…å®¹ï¼Œä½¿ç”¨RSSæ‘˜è¦: {article['title'][:50]}...")
                except Exception as e:
                    logger.warning(f"  âš ï¸  [{completed}/{len(articles_to_fetch)}] èŽ·å–å®Œæ•´å†…å®¹å¤±è´¥: {article['title'][:50]}... - {e}")
        
        logger.info(f"  âœ… å®Œæ•´å†…å®¹èŽ·å–å®Œæˆ: {len(articles_to_fetch)} ç¯‡æ–‡ç« ")
        return articles

    def _fix_source_by_feed_title(self, db, session, feed_title: str, correct_source_name: str):
        """
        æ ¹æ®feed titleä¿®æ­£æ•°æ®åº“ä¸­æ–‡ç« çš„sourceå­—æ®µ
        
        Args:
            db: æ•°æ®åº“ç®¡ç†å™¨
            session: æ•°æ®åº“ä¼šè¯
            feed_title: RSS feedçš„title
            correct_source_name: æ­£ç¡®çš„è®¢é˜…æºåç§°
        """
        try:
            # æŸ¥æ‰¾sourceå­—æ®µç­‰äºŽfeed_titleçš„æ–‡ç« 
            articles_to_fix = session.query(Article).filter(
                Article.source == feed_title
            ).all()
            
            if articles_to_fix:
                fixed_count = 0
                for article in articles_to_fix:
                    article.source = correct_source_name
                    fixed_count += 1
                
                session.commit()
                logger.info(f"  ðŸ”§ å·²ä¿®æ­£ {fixed_count} ç¯‡æ–‡ç« çš„sourceå­—æ®µ: '{feed_title}' -> '{correct_source_name}'")
        except Exception as e:
            logger.warning(f"  âš ï¸  ä¿®æ­£sourceå­—æ®µå¤±è´¥: {e}")
            session.rollback()

    def _update_task_progress(self, db, task_id: int, stats: Dict[str, Any]):
        """æ›´æ–°ä»»åŠ¡è¿›åº¦"""
        try:
            from database.models import CollectionTask
            with db.get_session() as session:
                task = session.query(CollectionTask).filter(CollectionTask.id == task_id).first()
                if task:
                    task.new_articles_count = stats.get('new_articles', 0)
                    task.total_sources = stats.get('sources_success', 0) + stats.get('sources_error', 0)
                    task.success_sources = stats.get('sources_success', 0)
                    task.failed_sources = stats.get('sources_error', 0)
                    task.ai_analyzed_count = stats.get('analyzed_count', 0)
                    session.commit()
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°ä»»åŠ¡è¿›åº¦å¤±è´¥: {e}")

    def _process_single_rss_source(self, db, source_name: str, feed_result: Dict[str, Any], enable_ai_analysis: bool = False) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªRSSæºï¼šèŽ·å–å®Œæ•´å†…å®¹ -> ä¿å­˜æ–‡ç«  -> AIåˆ†æžï¼ˆå…¨æµç¨‹å¹¶å‘ï¼‰

        Args:
            db: æ•°æ®åº“ç®¡ç†å™¨
            source_name: è®¢é˜…æºåç§°
            feed_result: RSSé‡‡é›†ç»“æžœ
            enable_ai_analysis: æ˜¯å¦å¯ç”¨AIåˆ†æž

        Returns:
            å¤„ç†ç»“æžœç»Ÿè®¡
        """
        result_stats = {
            "source_name": source_name,
            "total_articles": 0,
            "new_articles": 0,
            "skipped_articles": 0,  # å·²å­˜åœ¨çš„æ–‡ç« 
            "ai_analyzed": 0,
            "ai_skipped": 0,  # å·²åˆ†æžçš„æ–‡ç« 
            "success": False,
            "error": None
        }

        try:
            articles = feed_result.get("articles", [])
            feed_title = feed_result.get("feed_title")

            if not articles:
                result_stats["success"] = True
                return result_stats

            # å¦‚æžœfeed titleä¸Žè®¢é˜…æºåç§°ä¸ä¸€è‡´ï¼Œä¿®æ­£æ•°æ®åº“ä¸­å·²æœ‰çš„æ–‡ç« 
            if feed_title and feed_title != source_name:
                with db.get_session() as session:
                    self._fix_source_by_feed_title(db, session, feed_title, source_name)

            # ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„sourceåç§°
            for article in articles:
                article["source"] = source_name

            logger.info(f"  ðŸ“¥ {source_name}: å¼€å§‹å¤„ç† {len(articles)} ç¯‡æ–‡ç« ...")

            # ç¬¬ä¸€æ­¥ï¼šæ‰¹é‡æ£€æŸ¥å“ªäº›æ–‡ç« å·²å­˜åœ¨ä¸”æœ‰å†…å®¹ã€å·²åˆ†æž
            existing_articles_data = {}
            with db.get_session() as session:
                # æŸ¥è¯¢å·²å­˜åœ¨çš„æ–‡ç« ï¼ˆåŒ…æ‹¬å†…å®¹å’Œåˆ†æžçŠ¶æ€ï¼‰
                url_list = [article.get("url") for article in articles if article.get("url")]
                if url_list:
                    existing = session.query(
                        Article.url,
                        Article.content,
                        Article.is_processed
                    ).filter(Article.url.in_(url_list)).all()

                    # å­˜å‚¨æ¯ä¸ªURLçš„çŠ¶æ€ï¼š{"url": {"has_content": bool, "is_processed": bool}}
                    for row in existing:
                        existing_articles_data[row[0]] = {
                            "has_content": bool(row[1] and row[1].strip()),  # æ£€æŸ¥å†…å®¹æ˜¯å¦éžç©º
                            "is_processed": row[2]
                        }

            # ç¬¬äºŒæ­¥ï¼šåˆ†ç±»æ–‡ç« 
            articles_to_fetch = []  # éœ€è¦èŽ·å–å†…å®¹çš„æ–‡ç« 
            articles_to_analyze = []  # éœ€è¦AIåˆ†æžçš„æ–‡ç« 
            skipped_count = 0  # å®Œå…¨è·³è¿‡çš„æ–‡ç« 

            for article in articles:
                url = article.get("url")
                if not url:
                    continue

                if url not in existing_articles_data:
                    # æ–°æ–‡ç« ï¼Œéœ€è¦èŽ·å–å†…å®¹å’ŒAIåˆ†æž
                    articles_to_fetch.append(article)
                else:
                    # æ–‡ç« å·²å­˜åœ¨ï¼Œæ£€æŸ¥å†…å®¹å’Œåˆ†æžçŠ¶æ€
                    status = existing_articles_data[url]

                    if not status["has_content"]:
                        # å†…å®¹ä¸ºç©ºï¼Œéœ€è¦é‡æ–°èŽ·å–
                        articles_to_fetch.append(article)

                    if not status["is_processed"]:
                        # æœªåˆ†æžï¼Œéœ€è¦é‡æ–°åˆ†æžï¼ˆè®°å½•URLä»¥ä¾¿åŽç»­æŸ¥æ‰¾IDï¼‰
                        articles_to_analyze.append(article)

                    if status["has_content"] and status["is_processed"]:
                        # å†…å®¹å®Œæ•´ä¸”å·²åˆ†æžï¼Œå®Œå…¨è·³è¿‡
                        skipped_count += 1

            result_stats["total_articles"] = len(articles)
            result_stats["skipped_articles"] = skipped_count

            fetch_count = len(articles_to_fetch)
            analyze_count = len(articles_to_analyze)

            if skipped_count > 0 or fetch_count > 0 or analyze_count > 0:
                logger.info(f"  ðŸ“Š {source_name}: è·³è¿‡ {skipped_count} ç¯‡å®Œæ•´æ–‡ç« , éœ€èŽ·å–å†…å®¹ {fetch_count} ç¯‡, éœ€AIåˆ†æž {analyze_count} ç¯‡")

            if not articles_to_fetch and not articles_to_analyze:
                logger.info(f"  âœ… {source_name}: æ‰€æœ‰æ–‡ç« éƒ½å·²å®Œæ•´é‡‡é›†å’Œåˆ†æž")
                result_stats["success"] = True
                return result_stats

            logger.info(f"  ðŸ“¥ {source_name}: èŽ·å– {len(articles_to_fetch)} ç¯‡æ–‡ç« çš„å®Œæ•´å†…å®¹...")

            # ç¬¬ä¸‰æ­¥ï¼šå¹¶å‘èŽ·å–å®Œæ•´å†…å®¹ï¼ˆ3ä¸ªå¹¶å‘ï¼‰
            articles_with_full_content = self._fetch_articles_full_content(
                articles_to_fetch, source_name, max_workers=3
            )

            # ç¬¬å››æ­¥ï¼šä¿å­˜æˆ–æ›´æ–°æ–‡ç« åˆ°æ•°æ®åº“
            logger.info(f"  ðŸ’¾ {source_name}: å¼€å§‹ä¿å­˜æ–‡ç« ...")
            saved_article_ids = []
            updated_count = 0  # æ›´æ–°çš„æ–‡ç« æ•°ï¼ˆå·²æœ‰URLä½†è¡¥å……äº†å†…å®¹ï¼‰
            new_count = 0  # æ–°å¢žæ–‡ç« æ•°

            for article in articles_with_full_content:
                result = self._save_or_update_article_and_get_id(db, article)
                if result:
                    saved_article_ids.append(result)
                    if result["is_new"]:
                        new_count += 1
                    else:
                        updated_count += 1

            result_stats["new_articles"] = new_count

            # æ›´æ–°RSSæºçš„ç»Ÿè®¡ä¿¡æ¯
            with db.get_session() as session:
                source_obj = session.query(RSSSource).filter(RSSSource.name == source_name).first()
                if source_obj:
                    source_obj.last_collected_at = datetime.now()
                    source_obj.articles_count += len(articles)
                    source_obj.last_error = None
                    session.commit()

            # è®°å½•é‡‡é›†æ—¥å¿—
            self._log_collection(db, source_name, "rss", "success", len(articles))

            # ç¬¬äº”æ­¥ï¼šå¦‚æžœå¯ç”¨AIåˆ†æžï¼Œå¤„ç†éœ€è¦åˆ†æžçš„æ–‡ç« ï¼ˆåŒ…æ‹¬æ–°æ–‡ç« å’Œæ—§æ–‡ç« ï¼‰
            if enable_ai_analysis and self.ai_analyzer and (saved_article_ids or articles_to_analyze):
                # æ”¶é›†æ‰€æœ‰éœ€è¦åˆ†æžçš„æ–‡ç« ID
                all_article_ids = saved_article_ids.copy()

                # å¯¹äºŽå·²æœ‰URLä½†æœªåˆ†æžçš„æ–‡ç« ï¼ŒæŸ¥è¯¢å®ƒä»¬çš„ID
                if articles_to_analyze:
                    with db.get_session() as session:
                        for article in articles_to_analyze:
                            existing = session.query(Article.id).filter(Article.url == article.get("url")).first()
                            if existing:
                                all_article_ids.append(existing.id)

                # æ£€æŸ¥å“ªäº›æ–‡ç« å·²ç»åˆ†æžè¿‡äº†
                unanalyzed_ids = self._filter_unanalyzed_articles(db, all_article_ids)
                ai_skipped = len(all_article_ids) - len(unanalyzed_ids)

                if ai_skipped > 0:
                    logger.info(f"  â­ï¸  {source_name}: è·³è¿‡ {ai_skipped} ç¯‡å·²åˆ†æžçš„æ–‡ç« ")

                if unanalyzed_ids:
                    logger.info(f"  ðŸ¤– {source_name}: å¼€å§‹AIåˆ†æž {len(unanalyzed_ids)} ç¯‡æ–‡ç« ...")
                    analyzed_count = self._analyze_articles_by_ids(db, unanalyzed_ids, max_workers=3)
                    result_stats["ai_analyzed"] = analyzed_count

                result_stats["ai_skipped"] = ai_skipped

            result_stats["success"] = True
            logger.info(f"  âœ… {source_name}: æ€»å…± {len(articles)} ç¯‡, è·³è¿‡ {skipped_count} ç¯‡, æ–°å¢ž {new_count} ç¯‡, æ›´æ–° {updated_count} ç¯‡, AIåˆ†æž {result_stats['ai_analyzed']} ç¯‡")

        except Exception as e:
            logger.error(f"  âŒ {source_name}: {e}")
            result_stats["error"] = str(e)

            # æ›´æ–°é”™è¯¯ä¿¡æ¯
            with db.get_session() as session:
                source_obj = session.query(RSSSource).filter(RSSSource.name == source_name).first()
                if source_obj:
                    source_obj.last_error = str(e)
                    session.commit()

            self._log_collection(db, source_name, "rss", "error", 0, str(e))

        return result_stats

    def _collect_rss_sources(self, db, task_id: int = None, enable_ai_analysis: bool = False) -> Dict[str, Any]:
        """
        é‡‡é›†RSSæºï¼ˆåŒå±‚å¹¶å‘ï¼šå¤šä¸ªRSSæºåŒæ—¶é‡‡é›† + æ¯ä¸ªæºå†…éƒ¨å¹¶å‘èŽ·å–å†…å®¹+AIåˆ†æžï¼‰

        Args:
            db: æ•°æ®åº“ç®¡ç†å™¨
            task_id: ä»»åŠ¡ID
            enable_ai_analysis: æ˜¯å¦åœ¨é‡‡é›†æ¯ä¸ªæºåŽç«‹å³è¿›è¡ŒAIåˆ†æž

        Returns:
            é‡‡é›†ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {"sources_success": 0, "sources_error": 0, "new_articles": 0, "total_articles": 0, "ai_analyzed_count": 0}

        # ä¼˜å…ˆä»Žæ•°æ®åº“è¯»å–RSSæº
        rss_configs = []
        with db.get_session() as session:
            db_sources = session.query(RSSSource).filter(RSSSource.enabled == True).order_by(RSSSource.priority.asc()).all()

            for source in db_sources:
                rss_configs.append({
                    "name": source.name,
                    "url": source.url,
                    "enabled": source.enabled,
                    "max_articles": 20,  # é»˜è®¤å€¼
                    "category": source.category,
                    "tier": source.tier,
                })
                # é¢„å…ˆåŠ è½½å±žæ€§
                _ = source.id
                _ = source.name
                _ = source.url
                _ = source.enabled
                _ = source.last_collected_at
                _ = source.articles_count
            session.expunge_all()

        # å¦‚æžœæ•°æ®åº“ä¸­æ²¡æœ‰æºï¼Œåˆ™ä»Žé…ç½®æ–‡ä»¶è¯»å–ï¼ˆå‘åŽå…¼å®¹ï¼‰
        if not rss_configs:
            logger.info("  â„¹ï¸  æ•°æ®åº“ä¸­æ²¡æœ‰RSSæºï¼Œä»Žé…ç½®æ–‡ä»¶è¯»å–")
            rss_configs = self.config.get("rss_sources", [])

        if not rss_configs:
            logger.warning("  âš ï¸  æ²¡æœ‰é…ç½®RSSæº")
            return stats

        logger.info(f"  ðŸš€ å¼€å§‹é‡‡é›† {len(rss_configs)} ä¸ªRSSæºï¼ˆç¬¬ä¸€å±‚å¹¶å‘ï¼‰")

        # ç¬¬ä¸€å±‚å¹¶å‘ï¼šåŒæ—¶é‡‡é›†å¤šä¸ªRSSæº
        with ThreadPoolExecutor(max_workers=5) as executor:
            # æäº¤æ‰€æœ‰RSSé‡‡é›†ä»»åŠ¡
            future_to_source = {}

            for rss_config in rss_configs:
                source_name = rss_config["name"]

                # å®šä¹‰é‡‡é›†å•ä¸ªæºçš„å‡½æ•°
                def collect_single_source():
                    try:
                        # èŽ·å–RSS feed
                        feed_data = self.rss_collector.fetch_single_feed(rss_config)

                        # å¤„ç†è¿™ä¸ªæºï¼ˆåŒ…å«èŽ·å–å®Œæ•´å†…å®¹ã€ä¿å­˜ã€AIåˆ†æžï¼‰
                        result = self._process_single_rss_source(
                            db, source_name, feed_data, enable_ai_analysis
                        )
                        return result
                    except Exception as e:
                        logger.error(f"  âŒ {source_name} é‡‡é›†å¤±è´¥: {e}")
                        return {
                            "source_name": source_name,
                            "success": False,
                            "error": str(e),
                            "total_articles": 0,
                            "new_articles": 0,
                            "ai_analyzed": 0
                        }

                # æäº¤ä»»åŠ¡åˆ°çº¿ç¨‹æ± 
                future = executor.submit(collect_single_source)
                future_to_source[future] = source_name

            # æ”¶é›†ç»“æžœ
            completed = 0
            for future in as_completed(future_to_source):
                source_name = future_to_source[future]
                completed += 1

                try:
                    result = future.result()

                    if result["success"]:
                        stats["sources_success"] += 1
                        stats["new_articles"] += result["new_articles"]
                        stats["total_articles"] += result["total_articles"]
                        stats["ai_analyzed_count"] += result.get("ai_analyzed", 0)
                    else:
                        stats["sources_error"] += 1
                        logger.error(f"  âŒ {source_name}: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

                except Exception as e:
                    logger.error(f"  âŒ {source_name} å¤„ç†å¼‚å¸¸: {e}")
                    stats["sources_error"] += 1

        logger.info(f"  âœ… RSSé‡‡é›†å®Œæˆ: æˆåŠŸ {stats['sources_success']} ä¸ªæº, å¤±è´¥ {stats['sources_error']} ä¸ªæº")
        logger.info(f"     æ€»æ–‡ç« : {stats['total_articles']} ç¯‡, æ–°å¢ž: {stats['new_articles']} ç¯‡, AIåˆ†æž: {stats['ai_analyzed_count']} ç¯‡")

        return stats

    def _collect_api_sources(self, db, task_id: int = None) -> Dict[str, Any]:
        """é‡‡é›†APIæº"""
        stats = {"sources_success": 0, "sources_error": 0, "new_articles": 0, "total_articles": 0}

        api_configs = self.config.get("api_sources", [])

        for config in api_configs:
            if not config.get("enabled", True):
                continue

            name = config.get("name")
            source_type = config.get("category")

            try:
                articles = []
                if "arxiv" in name.lower():
                    query = config.get("query")
                    max_results = config.get("max_results", 20)
                    articles = self.arxiv_collector.fetch_papers(query, max_results)

                elif "hugging Face" in name.lower():
                    limit = config.get("max_results", 20)
                    articles = self.hf_collector.fetch_trending_papers(limit)

                elif "papers with code" in name.lower():
                    limit = config.get("max_results", 20)
                    articles = self.pwc_collector.fetch_trending_papers(limit)

                # ä¿å­˜æ–‡ç« 
                new_count = 0
                for article in articles:
                    if self._save_article(db, article):
                        new_count += 1

                # è®°å½•æ—¥å¿—
                self._log_collection(db, name, "api", "success", len(articles))
                stats["sources_success"] += 1
                stats["new_articles"] += new_count
                stats["total_articles"] += len(articles)

                logger.info(f"  âœ… {name}: {len(articles)} ç¯‡, æ–°å¢ž {new_count} ç¯‡")

            except Exception as e:
                logger.error(f"  âŒ {name}: {e}")
                self._log_collection(db, name, "api", "error", 0, str(e))
                stats["sources_error"] += 1

        return stats

    def _save_article(self, db, article: Dict[str, Any]) -> bool:
        """
        ä¿å­˜æ–‡ç« åˆ°æ•°æ®åº“

        Returns:
            True if new article, False if already exists
        """
        try:
            with db.get_session() as session:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                existing = session.query(Article).filter(Article.url == article["url"]).first()

                if existing:
                    return False

                # åˆ›å»ºæ–°æ–‡ç« 
                # å¯¹äºŽå®Œæ•´å†…å®¹ï¼Œä¸é™åˆ¶é•¿åº¦ï¼ˆä½¿ç”¨Textç±»åž‹å¯ä»¥å­˜å‚¨å¤§é‡æ–‡æœ¬ï¼‰
                content = article.get("content", "")
                new_article = Article(
                    title=article.get("title"),
                    url=article.get("url"),
                    content=content,  # ä¸é™åˆ¶é•¿åº¦ï¼Œä½¿ç”¨Textç±»åž‹
                    source=article.get("source"),
                    category=article.get("category"),
                    author=article.get("author"),
                    published_at=article.get("published_at"),
                    extra_data=article.get("metadata"),
                )

                session.add(new_article)
                session.commit()

                return True

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ–‡ç« å¤±è´¥: {e}")
            return False

    def _save_article_and_get_id(self, db, article: Dict[str, Any]) -> int or None:
        """
        ä¿å­˜æ–‡ç« åˆ°æ•°æ®åº“å¹¶è¿”å›žæ–‡ç« ID

        Returns:
            æ–‡ç« IDï¼ˆå¦‚æžœæ–‡ç« å·²å­˜åœ¨è¿”å›žNoneï¼‰
        """
        try:
            with db.get_session() as session:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                existing = session.query(Article).filter(Article.url == article["url"]).first()

                if existing:
                    return None

                # åˆ›å»ºæ–°æ–‡ç« 
                content = article.get("content", "")
                new_article = Article(
                    title=article.get("title"),
                    url=article.get("url"),
                    content=content,
                    source=article.get("source"),
                    category=article.get("category"),
                    author=article.get("author"),
                    published_at=article.get("published_at"),
                    extra_data=article.get("metadata"),
                )

                session.add(new_article)
                session.commit()

                # è¿”å›žæ–°æ’å…¥çš„æ–‡ç« ID
                return new_article.id

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ–‡ç« å¤±è´¥: {e}")
            return None

    def _save_or_update_article_and_get_id(self, db, article: Dict[str, Any]) -> Dict[str, Any] or None:
        """
        ä¿å­˜æˆ–æ›´æ–°æ–‡ç« åˆ°æ•°æ®åº“å¹¶è¿”å›žæ–‡ç« IDå’Œä¿¡æ¯

        Returns:
            {"id": int, "is_new": bool} - æ–‡ç« IDå’Œæ˜¯å¦ä¸ºæ–°æ–‡ç« 
            å¦‚æžœä¿å­˜å¤±è´¥è¿”å›žNone
        """
        try:
            with db.get_session() as session:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                existing = session.query(Article).filter(Article.url == article["url"]).first()

                if existing:
                    # æ–‡ç« å·²å­˜åœ¨ï¼Œæ›´æ–°å†…å®¹ï¼ˆå¦‚æžœæ–°å†…å®¹æ›´å®Œæ•´ï¼‰
                    content = article.get("content", "")
                    if content and content.strip():  # å¦‚æžœæœ‰æ–°å†…å®¹
                        # åªåœ¨å†…å®¹ä¸ºç©ºæˆ–æ˜Žæ˜¾æ›´çŸ­æ—¶æ‰æ›´æ–°
                        if not existing.content or (existing.content and len(content) > len(existing.content)):
                            existing.content = content
                            session.commit()
                            return {"id": existing.id, "is_new": False}
                    return {"id": existing.id, "is_new": False}

                # åˆ›å»ºæ–°æ–‡ç« 
                content = article.get("content", "")
                new_article = Article(
                    title=article.get("title"),
                    url=article.get("url"),
                    content=content,
                    source=article.get("source"),
                    category=article.get("category"),
                    author=article.get("author"),
                    published_at=article.get("published_at"),
                    extra_data=article.get("metadata"),
                )

                session.add(new_article)
                session.commit()

                # è¿”å›žæ–°æ’å…¥çš„æ–‡ç« ID
                return {"id": new_article.id, "is_new": True}

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æˆ–æ›´æ–°æ–‡ç« å¤±è´¥: {e}")
            return None

    def _analyze_articles(self, db, batch_size: int = 50, max_age_days: int = 3, max_workers: int = 3) -> Dict[str, Any]:
        """
        AIåˆ†æžæœªåˆ†æžçš„æ–‡ç« ï¼ˆå¹¶å‘ï¼‰
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            max_age_days: æœ€å¤§æ–‡ç« å¹´é¾„ï¼ˆå¤©æ•°ï¼‰ï¼Œè¶…è¿‡æ­¤å¤©æ•°çš„æ–‡ç« ä¸åˆ†æžï¼Œé»˜è®¤3å¤©
            max_workers: æœ€å¤§å¹¶å‘æ•°ï¼Œé»˜è®¤3
        """
        stats = {"analyzed_count": 0, "analysis_error": 0, "skipped_old": 0}

        with db.get_session() as session:
            # è®¡ç®—æ—¶é—´é˜ˆå€¼ï¼ˆåªåˆ†æžæœ€è¿‘max_age_dayså¤©çš„æ–‡ç« ï¼‰
            from datetime import timedelta
            time_threshold = datetime.now() - timedelta(days=max_age_days)
            
            # èŽ·å–æœªåˆ†æžçš„æ–‡ç« ï¼ˆåªåˆ†æžæœ€è¿‘çš„æ–‡ç« ï¼‰
            unanalyzed = (
                session.query(Article)
                .filter(
                    Article.is_processed == False,
                    Article.published_at.isnot(None),
                    Article.published_at >= time_threshold
                )
                .order_by(Article.published_at.desc())
                .limit(batch_size)
                .all()
            )
            
            # ç»Ÿè®¡è·³è¿‡çš„æ—§æ–‡ç« 
            skipped_count = (
                session.query(Article)
                .filter(
                    Article.is_processed == False,
                    Article.published_at.isnot(None),
                    Article.published_at < time_threshold
                )
                .count()
            )
            stats["skipped_old"] = skipped_count

            if not unanalyzed:
                if skipped_count > 0:
                    logger.info(f"  âœ… æ²¡æœ‰éœ€è¦AIåˆ†æžçš„æ–‡ç« ï¼ˆè·³è¿‡äº† {skipped_count} ç¯‡è¶…è¿‡ {max_age_days} å¤©çš„æ—§æ–‡ç« ï¼‰")
                else:
                    logger.info("  âœ… æ²¡æœ‰éœ€è¦AIåˆ†æžçš„æ–‡ç« ")
                return stats

            logger.info(f"  ðŸ¤– å¼€å§‹å¹¶å‘åˆ†æž {len(unanalyzed)} ç¯‡æ–‡ç« ï¼ˆæŒ‰æ—¶é—´ä»Žæ–°åˆ°æ—§æŽ’åºï¼Œæœ€å¤§å¹¶å‘æ•°: {max_workers}ï¼Œè·³è¿‡äº† {skipped_count} ç¯‡è¶…è¿‡ {max_age_days} å¤©çš„æ—§æ–‡ç« ï¼‰")
            
            # æ˜¾ç¤ºå°†è¦åˆ†æžçš„æ–‡ç« æ—¶é—´èŒƒå›´
            if unanalyzed:
                latest_date = unanalyzed[0].published_at
                oldest_date = unanalyzed[-1].published_at
                if latest_date and oldest_date:
                    logger.info(f"  ðŸ“… åˆ†æžæ—¶é—´èŒƒå›´: {oldest_date.strftime('%Y-%m-%d')} è‡³ {latest_date.strftime('%Y-%m-%d')}")

            # é¢„å…ˆåŠ è½½æ‰€æœ‰å±žæ€§ï¼Œé¿å…åœ¨å¹¶å‘æ—¶å‡ºçŽ°DetachedInstanceError
            for article in unanalyzed:
                _ = article.id
                _ = article.title
                _ = article.content
                _ = article.source
                _ = article.published_at
            
            session.expunge_all()

            # å¹¶å‘åˆ†æžæ–‡ç« 
            def analyze_single_article(article):
                """åˆ†æžå•ç¯‡æ–‡ç« ï¼ˆç”¨äºŽå¹¶å‘æ‰§è¡Œï¼‰"""
                try:
                    # ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„æ•°æ®åº“ä¼šè¯
                    with db.get_session() as article_session:
                        # é‡æ–°æŸ¥è¯¢æ–‡ç« ï¼ˆé¿å…DetachedInstanceErrorï¼‰
                        article_obj = article_session.query(Article).filter(Article.id == article.id).first()
                        if not article_obj or article_obj.is_processed:
                            return {"success": False, "reason": "already_processed"}
                        
                        # å‡†å¤‡æ–‡ç« æ•°æ®
                        article_dict = {
                            "title": article_obj.title,
                            "content": article_obj.content,
                            "source": article_obj.source,
                            "published_at": article_obj.published_at,
                        }

                        # AIåˆ†æž
                        result = self.ai_analyzer.analyze_article(article_dict)

                        # æ›´æ–°æ–‡ç« 
                        article_obj.summary = result.get("summary")
                        article_obj.topics = result.get("topics")
                        article_obj.tags = result.get("tags")
                        article_obj.importance = result.get("importance")
                        article_obj.target_audience = result.get("target_audience")
                        article_obj.key_points = result.get("key_points")
                        article_obj.is_processed = True

                        article_session.commit()
                        return {"success": True, "article_id": article_obj.id}
                        
                except Exception as e:
                    logger.error(f"  âŒ åˆ†æžæ–‡ç« å¤±è´¥ (ID={article.id}): {e}")
                    return {"success": False, "error": str(e)}

            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘åˆ†æž
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_article = {
                    executor.submit(analyze_single_article, article): article
                    for article in unanalyzed
                }
                
                completed = 0
                for future in as_completed(future_to_article):
                    article = future_to_article[future]
                    completed += 1
                    
                    try:
                        result = future.result()
                        if result.get("success"):
                            stats["analyzed_count"] += 1
                            if completed % 5 == 0 or completed == len(unanalyzed):
                                logger.info(f"  âœ… [{completed}/{len(unanalyzed)}] AIåˆ†æžè¿›åº¦")
                        else:
                            stats["analysis_error"] += 1
                    except Exception as e:
                        logger.error(f"  âŒ åˆ†æžæ–‡ç« å¼‚å¸¸ (ID={article.id}): {e}")
                        stats["analysis_error"] += 1

        logger.info(f"  âœ… AIåˆ†æžå®Œæˆ: {stats['analyzed_count']} ç¯‡æˆåŠŸ, {stats['analysis_error']} ç¯‡å¤±è´¥")
        return stats

    def _analyze_articles_by_ids(self, db, article_ids: List[int], max_workers: int = 3) -> int:
        """
        æ ¹æ®æ–‡ç« IDåˆ—è¡¨è¿›è¡Œå¹¶å‘AIåˆ†æž

        Args:
            db: æ•°æ®åº“ç®¡ç†å™¨
            article_ids: æ–‡ç« IDåˆ—è¡¨
            max_workers: æœ€å¤§å¹¶å‘æ•°

        Returns:
            æˆåŠŸåˆ†æžçš„æ–‡ç« æ•°é‡
        """
        if not article_ids or not self.ai_analyzer:
            return 0

        analyzed_count = 0

        # å¹¶å‘åˆ†æžæ–‡ç« 
        def analyze_single_article_id(article_id):
            """æ ¹æ®IDåˆ†æžå•ç¯‡æ–‡ç« """
            try:
                # ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„æ•°æ®åº“ä¼šè¯
                with db.get_session() as session:
                    # é‡æ–°æŸ¥è¯¢æ–‡ç« 
                    article_obj = session.query(Article).filter(Article.id == article_id).first()
                    if not article_obj or article_obj.is_processed:
                        return {"success": False, "reason": "already_processed"}

                    # å‡†å¤‡æ–‡ç« æ•°æ®
                    article_dict = {
                        "title": article_obj.title,
                        "content": article_obj.content,
                        "source": article_obj.source,
                        "published_at": article_obj.published_at,
                    }

                    # AIåˆ†æž
                    result = self.ai_analyzer.analyze_article(article_dict)

                    # æ›´æ–°æ–‡ç« 
                    article_obj.summary = result.get("summary")
                    article_obj.topics = result.get("topics")
                    article_obj.tags = result.get("tags")
                    article_obj.importance = result.get("importance")
                    article_obj.target_audience = result.get("target_audience")
                    article_obj.key_points = result.get("key_points")
                    article_obj.is_processed = True

                    session.commit()
                    return {"success": True, "article_id": article_obj.id}

            except Exception as e:
                logger.error(f"  âŒ åˆ†æžæ–‡ç« å¤±è´¥ (ID={article_id}): {e}")
                return {"success": False, "error": str(e)}

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘åˆ†æž
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_id = {
                executor.submit(analyze_single_article_id, article_id): article_id
                for article_id in article_ids
            }

            completed = 0
            for future in as_completed(future_to_id):
                article_id = future_to_id[future]
                completed += 1

                try:
                    result = future.result()
                    if result.get("success"):
                        analyzed_count += 1
                        if completed % 5 == 0 or completed == len(article_ids):
                            logger.info(f"  âœ… [{completed}/{len(article_ids)}] AIåˆ†æžè¿›åº¦")
                except Exception as e:
                    logger.error(f"  âŒ åˆ†æžæ–‡ç« å¼‚å¸¸ (ID={article_id}): {e}")

        logger.info(f"  âœ… AIåˆ†æžå®Œæˆ: {analyzed_count} ç¯‡")
        return analyzed_count

    def _filter_unanalyzed_articles(self, db, article_ids: List[int]) -> List[int]:
        """
        è¿‡æ»¤å‡ºæœªåˆ†æžçš„æ–‡ç« IDåˆ—è¡¨

        Args:
            db: æ•°æ®åº“ç®¡ç†å™¨
            article_ids: æ–‡ç« IDåˆ—è¡¨

        Returns:
            æœªåˆ†æžçš„æ–‡ç« IDåˆ—è¡¨
        """
        if not article_ids:
            return []

        try:
            with db.get_session() as session:
                # æŸ¥è¯¢æœªåˆ†æžçš„æ–‡ç« 
                unanalyzed = session.query(Article.id).filter(
                    Article.id.in_(article_ids),
                    Article.is_processed == False
                ).all()

                return [row[0] for row in unanalyzed]
        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢æœªåˆ†æžæ–‡ç« å¤±è´¥: {e}")
            return article_ids  # å¦‚æžœæŸ¥è¯¢å¤±è´¥ï¼Œè¿”å›žæ‰€æœ‰IDç»§ç»­å¤„ç†

    def _log_collection(self, db, source_name: str, source_type: str, status: str, count: int, error: str = None):
        """è®°å½•é‡‡é›†æ—¥å¿—"""
        try:
            with db.get_session() as session:
                log = CollectionLog(
                    source_name=source_name,
                    source_type=source_type,
                    status=status,
                    articles_count=count,
                    error_message=error,
                )
                session.add(log)
                session.commit()
        except Exception as e:
            logger.error(f"âŒ è®°å½•æ—¥å¿—å¤±è´¥: {e}")

    def get_recent_articles(self, db, limit: int = 100, hours: int = 24) -> List[Article]:
        """èŽ·å–æœ€è¿‘çš„æ–‡ç« """
        with db.get_session() as session:
            time_threshold = datetime.now() - timedelta(hours=hours)

            articles = (
                session.query(Article)
                .filter(Article.published_at >= time_threshold)
                .order_by(desc(Article.published_at))
                .limit(limit)
                .all()
            )

            return articles

    def get_daily_summary(self, db, limit: int = 10) -> List[Article]:
        """èŽ·å–æ¯æ—¥é‡è¦æ–‡ç« """
        with db.get_session() as session:
            time_threshold = datetime.now() - timedelta(hours=24)

            articles = (
                session.query(Article)
                .filter(Article.published_at >= time_threshold, Article.importance.in_(["high", "medium"]))
                .order_by(desc(Article.published_at))
                .limit(limit)
                .all()
            )

            return articles
