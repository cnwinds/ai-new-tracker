"""
RAGæœåŠ¡ - å®ç°æ–‡ç« å‘é‡ç´¢å¼•ã€æœç´¢å’Œé—®ç­”åŠŸèƒ½
"""
import json
import logging
import numpy as np
import struct
from typing import List, Dict, Any, Optional
from datetime import datetime
from sqlalchemy.orm import Session
from sqlalchemy import and_, or_, text
from sqlalchemy.engine import Connection

from backend.app.db.models import Article, ArticleEmbedding
from backend.app.services.analyzer.ai_analyzer import AIAnalyzer

logger = logging.getLogger(__name__)


class RAGService:
    """RAGæœåŠ¡ç±»"""

    def __init__(self, ai_analyzer: AIAnalyzer, db: Session):
        """
        åˆå§‹åŒ–RAGæœåŠ¡

        Args:
            ai_analyzer: AIåˆ†æå™¨å®ä¾‹ï¼ˆç”¨äºç”ŸæˆåµŒå…¥å‘é‡ï¼‰
            db: æ•°æ®åº“ä¼šè¯
        """
        self.ai_analyzer = ai_analyzer
        self.db = db
        self._use_sqlite_vec = self._check_sqlite_vec_available()

    def _check_sqlite_vec_available(self) -> bool:
        """æ£€æŸ¥sqlite-vecæ‰©å±•æ˜¯å¦å¯ç”¨"""
        try:
            # å°è¯•æŸ¥è¯¢vec0è™šæ‹Ÿè¡¨
            result = self.db.execute(text("SELECT 1 FROM vec_embeddings LIMIT 1"))
            result.fetchone()
            logger.debug("âœ… sqlite-vecæ‰©å±•å¯ç”¨ï¼Œå°†ä½¿ç”¨SQLå‘é‡æœç´¢")
            return True
        except Exception as e:
            logger.debug(f"â„¹ï¸  sqlite-vecæ‰©å±•ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨Pythonå‘é‡è®¡ç®—: {e}")
            return False

    def _vector_to_blob(self, vector: List[float]) -> bytes:
        """å°†å‘é‡è½¬æ¢ä¸ºBLOBæ ¼å¼ï¼ˆsqlite-vecéœ€è¦ï¼‰"""
        # sqlite-vecæœŸæœ›çš„æ ¼å¼ï¼šæµ®ç‚¹æ•°æ•°ç»„ï¼ˆå°ç«¯åºï¼‰
        return struct.pack(f'{len(vector)}f', *vector)

    def _vector_to_match_string(self, vector: List[float]) -> str:
        """å°†å‘é‡è½¬æ¢ä¸ºMATCHæ“ä½œç¬¦éœ€è¦çš„å­—ç¬¦ä¸²æ ¼å¼"""
        # sqlite-vecçš„MATCHæ“ä½œç¬¦éœ€è¦JSONæ•°ç»„æ ¼å¼çš„å­—ç¬¦ä¸²
        return json.dumps(vector)

    def _combine_article_text(self, article: Article) -> str:
        """
        ç»„åˆæ–‡ç« çš„æ‰€æœ‰å­—æ®µä¸ºç´¢å¼•æ–‡æœ¬

        Args:
            article: æ–‡ç« å¯¹è±¡

        Returns:
            ç»„åˆåçš„æ–‡æœ¬
        """
        parts = []
        
        # æ ‡é¢˜
        if article.title:
            parts.append(f"æ ‡é¢˜: {article.title}")
        
        # ä¸­æ–‡æ ‡é¢˜
        if article.title_zh:
            parts.append(f"ä¸­æ–‡æ ‡é¢˜: {article.title_zh}")
        
        # æ‘˜è¦
        if article.summary:
            parts.append(f"æ‘˜è¦: {article.summary}")
        
        # å†…å®¹ï¼ˆæˆªå–å‰2000å­—ç¬¦ï¼Œçº¦2000 tokensï¼Œç¬¦åˆæœ€ä½³å®è·µ256-512 tokensçš„4å€èŒƒå›´ï¼‰
        # å¦‚æœå·²æœ‰æ‘˜è¦ï¼Œå†…å®¹ä½œä¸ºè¡¥å……ä¿¡æ¯ï¼Œä¸éœ€è¦å¤ªé•¿
        if article.content:
            # ä¼˜å…ˆä½¿ç”¨æ‘˜è¦ï¼Œå¦‚æœæ‘˜è¦å­˜åœ¨ï¼Œå†…å®¹åªå–å‰2000å­—ç¬¦ä½œä¸ºè¡¥å……
            # å¦‚æœæ‘˜è¦ä¸å­˜åœ¨ï¼Œåˆ™å–å‰3000å­—ç¬¦
            max_content_length = 2000 if article.summary else 3000
            content_preview = article.content[:max_content_length]
            parts.append(f"å†…å®¹: {content_preview}")
        
        # æ ‡ç­¾
        if article.tags:
            if isinstance(article.tags, list):
                tags_str = "ã€".join(article.tags)
                parts.append(f"æ ‡ç­¾: {tags_str}")
        
        # æ¥æº
        if article.source:
            parts.append(f"æ¥æº: {article.source}")
        
        combined_text = "\n\n".join(parts)
        return combined_text if combined_text.strip() else ""

    def generate_embedding(self, text: str) -> List[float]:
        """
        ç”Ÿæˆæ–‡æœ¬çš„åµŒå…¥å‘é‡

        Args:
            text: è¦ç”ŸæˆåµŒå…¥å‘é‡çš„æ–‡æœ¬

        Returns:
            åµŒå…¥å‘é‡åˆ—è¡¨
        """
        if not text or not text.strip():
            logger.warning("âš ï¸  ç”ŸæˆåµŒå…¥å‘é‡æ—¶æ–‡æœ¬ä¸ºç©º")
            return []
        
        return self.ai_analyzer.generate_embedding(text)

    def index_article(self, article: Article) -> bool:
        """
        ç´¢å¼•å•ç¯‡æ–‡ç« 

        Args:
            article: æ–‡ç« å¯¹è±¡

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            # æ£€æŸ¥æ˜¯å¦å·²ç´¢å¼•
            existing = self.db.query(ArticleEmbedding).filter(
                ArticleEmbedding.article_id == article.id
            ).first()
            
            if existing:
                logger.debug(f"æ–‡ç«  {article.id} å·²å­˜åœ¨ç´¢å¼•ï¼Œå°†æ›´æ–°")
            
            # ç”Ÿæˆç´¢å¼•æ–‡æœ¬
            text_content = self._combine_article_text(article)
            if not text_content.strip():
                logger.warning(f"âš ï¸  æ–‡ç«  {article.id} æ²¡æœ‰å¯ç´¢å¼•çš„å†…å®¹")
                return False
            
            # ç”ŸæˆåµŒå…¥å‘é‡
            logger.info(f"ğŸ“ æ­£åœ¨ä¸ºæ–‡ç«  {article.id} ç”ŸæˆåµŒå…¥å‘é‡...")
            embedding = self.generate_embedding(text_content)
            
            if not embedding:
                logger.error(f"âŒ æ–‡ç«  {article.id} åµŒå…¥å‘é‡ç”Ÿæˆå¤±è´¥")
                return False
            
            # ä¿å­˜æˆ–æ›´æ–°
            if existing:
                existing.embedding = embedding
                existing.text_content = text_content
                existing.embedding_model = self.ai_analyzer.embedding_model
                existing.updated_at = datetime.now()
            else:
                embedding_obj = ArticleEmbedding(
                    article_id=article.id,
                    embedding=embedding,
                    text_content=text_content,
                    embedding_model=self.ai_analyzer.embedding_model
                )
                self.db.add(embedding_obj)
            
            self.db.commit()
            
            # å¦‚æœsqlite-vecå¯ç”¨ï¼ŒåŒæ­¥åˆ°vec0è™šæ‹Ÿè¡¨
            if self._use_sqlite_vec:
                try:
                    # sqlite-vecçš„vec0è¡¨éœ€è¦JSONæ•°ç»„æ ¼å¼çš„å­—ç¬¦ä¸²
                    # æ ¼å¼: "[0.1, 0.2, 0.3, ...]"
                    vector_str = "[" + ",".join(map(str, embedding)) + "]"
                    
                    # è™šæ‹Ÿè¡¨å¯èƒ½ä¸æ”¯æŒ INSERT OR REPLACEï¼Œå…ˆåˆ é™¤å†æ’å…¥
                    # å…ˆåˆ é™¤æ—§è®°å½•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    self.db.execute(
                        text("DELETE FROM vec_embeddings WHERE article_id = :article_id"),
                        {"article_id": article.id}
                    )
                    
                    # æ’å…¥æ–°è®°å½•ï¼ˆä½¿ç”¨å­—ç¬¦ä¸²æ ¼å¼ï¼Œä¸åˆå§‹åŒ–ä»£ç ä¿æŒä¸€è‡´ï¼‰
                    self.db.execute(
                        text("""
                            INSERT INTO vec_embeddings (article_id, embedding)
                            VALUES (:article_id, :embedding)
                        """),
                        {"article_id": article.id, "embedding": vector_str}
                    )
                    self.db.commit()
                except Exception as e:
                    logger.warning(f"âš ï¸  åŒæ­¥å‘é‡åˆ°vec0è¡¨å¤±è´¥: {e}")
                    # è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯ä»¥ä¾¿è°ƒè¯•
                    import traceback
                    logger.debug(f"åŒæ­¥å‘é‡è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            
            logger.info(f"âœ… æ–‡ç«  {article.id} ç´¢å¼•æˆåŠŸ")
            return True
            
        except Exception as e:
            self.db.rollback()
            logger.error(f"âŒ æ–‡ç«  {article.id} ç´¢å¼•å¤±è´¥: {e}")
            return False

    def index_articles_batch(self, articles: List[Article], batch_size: int = 10) -> Dict[str, Any]:
        """
        æ‰¹é‡ç´¢å¼•æ–‡ç« 

        Args:
            articles: æ–‡ç« åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°

        Returns:
            ç»Ÿè®¡ä¿¡æ¯
        """
        total = len(articles)
        success_count = 0
        fail_count = 0
        
        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡ç´¢å¼• {total} ç¯‡æ–‡ç« ...")
        
        for i, article in enumerate(articles, 1):
            try:
                if self.index_article(article):
                    success_count += 1
                else:
                    fail_count += 1
                
                if i % batch_size == 0:
                    logger.info(f"ğŸ“Š è¿›åº¦: {i}/{total} (æˆåŠŸ: {success_count}, å¤±è´¥: {fail_count})")
                    
            except Exception as e:
                logger.error(f"âŒ æ‰¹é‡ç´¢å¼•æ–‡ç«  {article.id} æ—¶å‡ºé”™: {e}")
                fail_count += 1
        
        logger.info(f"âœ… æ‰¹é‡ç´¢å¼•å®Œæˆ: æ€»è®¡ {total}, æˆåŠŸ {success_count}, å¤±è´¥ {fail_count}")
        
        return {
            "total": total,
            "success": success_count,
            "failed": fail_count
        }

    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦

        Args:
            vec1: å‘é‡1
            vec2: å‘é‡2

        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0-1)ï¼Œå·²å½’ä¸€åŒ–
        """
        try:
            v1 = np.array(vec1)
            v2 = np.array(vec2)
            
            dot_product = np.dot(v1, v2)
            norm1 = np.linalg.norm(v1)
            norm2 = np.linalg.norm(v2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆèŒƒå›´ [-1, 1]ï¼‰
            cosine_sim = dot_product / (norm1 * norm2)
            
            # å½’ä¸€åŒ–åˆ° [0, 1] èŒƒå›´ï¼šsimilarity = (cosine_sim + 1) / 2
            # è¿™æ · -1 -> 0, 0 -> 0.5, 1 -> 1.0
            similarity = (cosine_sim + 1.0) / 2.0
            
            # ç¡®ä¿ç»“æœåœ¨ [0, 1] èŒƒå›´å†…ï¼ˆå¤„ç†æµ®ç‚¹è¯¯å·®ï¼‰
            similarity = max(0.0, min(1.0, similarity))
            
            return float(similarity)
        except Exception as e:
            logger.error(f"âŒ è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            return 0.0

    def search_articles(
        self,
        query: str,
        top_k: int = 10,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        è¯­ä¹‰æœç´¢æ–‡ç« 

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›å‰kä¸ªç»“æœ
            filters: è¿‡æ»¤æ¡ä»¶ï¼ˆsources, importance, time_rangeç­‰ï¼‰

        Returns:
            æœç´¢ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœåŒ…å«æ–‡ç« ä¿¡æ¯å’Œç›¸ä¼¼åº¦åˆ†æ•°
        """
        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            logger.info(f"ğŸ” æ­£åœ¨æœç´¢: {query[:50]}...")
            query_embedding = self.generate_embedding(query)
            
            if not query_embedding:
                logger.error("âŒ æŸ¥è¯¢å‘é‡ç”Ÿæˆå¤±è´¥")
                return []
            
            # å¦‚æœsqlite-vecå¯ç”¨ï¼Œä½¿ç”¨SQLå‘é‡æœç´¢
            if self._use_sqlite_vec:
                return self._search_with_sqlite_vec(query_embedding, top_k, filters)
            else:
                # å›é€€åˆ°Pythonå‘é‡è®¡ç®—
                return self._search_with_python(query_embedding, top_k, filters)
            
        except Exception as e:
            logger.error(f"âŒ æœç´¢å¤±è´¥: {e}")
            return []

    def _search_with_sqlite_vec(
        self,
        query_embedding: List[float],
        top_k: int,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """ä½¿ç”¨sqlite-vecè¿›è¡Œå‘é‡æœç´¢"""
        try:
            # æ£€æŸ¥vec_embeddingsè¡¨æ˜¯å¦æœ‰æ•°æ®
            vec_count = self.db.execute(text("SELECT COUNT(*) FROM vec_embeddings")).scalar()
            logger.debug(f"vec_embeddingsè¡¨ä¸­æœ‰ {vec_count} æ¡è®°å½•")
            if vec_count == 0:
                logger.warning("âš ï¸  vec_embeddingsè¡¨ä¸ºç©ºï¼Œå›é€€åˆ°Pythonè®¡ç®—")
                return self._search_with_python(query_embedding, top_k, filters)
            
            # æ£€æŸ¥æŸ¥è¯¢å‘é‡ç»´åº¦æ˜¯å¦ä¸æ•°æ®åº“ä¸­å­˜å‚¨çš„å‘é‡ç»´åº¦åŒ¹é…
            # ä»article_embeddingsè¡¨è·å–ä¸€ä¸ªæ ·æœ¬å‘é‡æ¥æ£€æŸ¥ç»´åº¦
            query_dim = len(query_embedding)
            sample_embedding = self.db.query(ArticleEmbedding).first()
            if sample_embedding and sample_embedding.embedding:
                stored_dim = len(sample_embedding.embedding)
                logger.debug(f"æŸ¥è¯¢å‘é‡ç»´åº¦: {query_dim}, å­˜å‚¨å‘é‡ç»´åº¦: {stored_dim}")
                if query_dim != stored_dim:
                    logger.warning(
                        f"âš ï¸  å‘é‡ç»´åº¦ä¸åŒ¹é…ï¼šæŸ¥è¯¢å‘é‡ç»´åº¦ {query_dim}ï¼Œ"
                        f"å­˜å‚¨å‘é‡ç»´åº¦ {stored_dim}ï¼Œå›é€€åˆ°Pythonè®¡ç®—"
                    )
                    return self._search_with_python(query_embedding, top_k, filters)
            else:
                logger.warning("âš ï¸  æœªæ‰¾åˆ°å·²ç´¢å¼•çš„æ–‡ç« å‘é‡ï¼Œå›é€€åˆ°Pythonè®¡ç®—")
                return self._search_with_python(query_embedding, top_k, filters)
            
            # sqlite-vecä½¿ç”¨MATCHæ“ä½œç¬¦ï¼Œéœ€è¦JSONæ•°ç»„æ ¼å¼çš„å­—ç¬¦ä¸²
            # æˆ–è€…å¯ä»¥ç›´æ¥ä½¿ç”¨BLOBæ ¼å¼
            query_vector_str = self._vector_to_match_string(query_embedding)
            
            # æ„å»ºåŸºç¡€æŸ¥è¯¢ - ä½¿ç”¨MATCHæ“ä½œç¬¦
            # vec0 çš„ MATCH éœ€è¦æ˜ç¡®æŒ‡å®š k å‚æ•°ï¼šMATCH ? AND k = 10
            # æ³¨æ„ï¼šk å‚æ•°å¿…é¡»å¤§äºç­‰äº top_kï¼Œæˆ‘ä»¬ä½¿ç”¨ top_k * 3 ä»¥ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç»“æœç”¨äºè¿‡æ»¤å’Œå»é‡
            # k å‚æ•°å¿…é¡»ç›´æ¥å†™åœ¨ SQL ä¸­ï¼Œä¸èƒ½ä½œä¸ºå‚æ•°ç»‘å®š
            k_value = max(top_k * 3, 20)  # è‡³å°‘è¿”å› 20 ä¸ªç»“æœï¼Œç¡®ä¿å»é‡åæœ‰è¶³å¤Ÿçš„ç»“æœ
            
            # æ„å»ºåŸºç¡€æŸ¥è¯¢ï¼ˆåŒ…å« is_favorited å­—æ®µç”¨äºæƒé‡è®¡ç®—ï¼‰
            sql = f"""
                SELECT 
                    v.article_id,
                    distance,
                    a.id, a.title, a.title_zh, a.url, a.summary, a.source,
                    a.published_at, a.importance, a.tags, a.is_favorited
                FROM vec_embeddings v
                JOIN articles a ON v.article_id = a.id
                WHERE v.embedding MATCH :query_vector AND k = {k_value}
            """
            
            params = {
                "query_vector": query_vector_str
            }
            
            # æ·»åŠ è¿‡æ»¤æ¡ä»¶
            if filters:
                conditions = []
                if filters.get("sources"):
                    placeholders = ",".join([f":source_{i}" for i in range(len(filters["sources"]))])
                    conditions.append(f"a.source IN ({placeholders})")
                    for i, source in enumerate(filters["sources"]):
                        params[f"source_{i}"] = source
                
                if filters.get("importance"):
                    placeholders = ",".join([f":importance_{i}" for i in range(len(filters["importance"]))])
                    conditions.append(f"a.importance IN ({placeholders})")
                    for i, imp in enumerate(filters["importance"]):
                        params[f"importance_{i}"] = imp
                
                if filters.get("time_from"):
                    conditions.append("a.published_at >= :time_from")
                    params["time_from"] = filters["time_from"]
                
                if filters.get("time_to"):
                    conditions.append("a.published_at <= :time_to")
                    params["time_to"] = filters["time_to"]
                
                if conditions:
                    sql += " AND " + " AND ".join(conditions)
            
            # æŒ‰è·ç¦»æ’åºï¼ˆè·ç¦»è¶Šå°è¶Šç›¸ä¼¼ï¼‰ï¼Œä¸åœ¨è¿™é‡Œé™åˆ¶æ•°é‡ï¼Œè®©å»é‡åå†é™åˆ¶
            # è¿™æ ·å¯ä»¥ç¡®ä¿å»é‡åæœ‰è¶³å¤Ÿçš„ç»“æœ
            sql += " ORDER BY distance"
            
            # æ‰§è¡ŒæŸ¥è¯¢
            result = self.db.execute(text(sql), params)
            rows = result.fetchall()
            
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            search_results = []
            for row in rows:
                distance = float(row[1]) if row[1] is not None else float('inf')
                
                if distance < float('inf'):
                    # åˆ¤æ–­è·ç¦»åº¦é‡ç±»å‹ï¼š
                    # - å¦‚æœè·ç¦» <= 2.0ï¼Œå¾ˆå¯èƒ½æ˜¯ä½™å¼¦è·ç¦»ï¼ˆä½™å¼¦è·ç¦»èŒƒå›´ [0, 2]ï¼‰
                    # - å¦‚æœè·ç¦» > 2.0ï¼Œå¾ˆå¯èƒ½æ˜¯ L2 è·ç¦»
                    if distance <= 2.0:
                        # ä½™å¼¦è·ç¦»ï¼šsimilarity = 1 - distance
                        # ä½™å¼¦è·ç¦» = 1 - ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œæ‰€ä»¥ç›¸ä¼¼åº¦ = 1 - distance
                        similarity = max(0.0, min(1.0, 1.0 - distance))
                    else:
                        # L2 è·ç¦»ï¼šä½¿ç”¨æ”¹è¿›çš„è½¬æ¢å…¬å¼
                        # å¯¹äºå½’ä¸€åŒ–çš„å‘é‡ï¼ŒL2 è·ç¦»èŒƒå›´æ˜¯ [0, 2]ï¼Œä½†å®é™…å¯èƒ½æ›´å¤§
                        # ä½¿ç”¨å¹³æ»‘çš„è½¬æ¢å‡½æ•°ï¼šsimilarity = 1 / (1 + distance)
                        # è¿™ç¡®ä¿è·ç¦»ä¸º 0 æ—¶ç›¸ä¼¼åº¦ä¸º 1ï¼Œè·ç¦»å¢å¤§æ—¶ç›¸ä¼¼åº¦é€’å‡
                        similarity = 1.0 / (1.0 + distance)
                else:
                    similarity = 0.0
                
                # å¦‚æœæ–‡ç« è¢«æ”¶è—ï¼Œå¢åŠ æƒé‡ï¼ˆæå‡ç›¸ä¼¼åº¦åˆ†æ•°ï¼‰
                is_favorited = row[12] if len(row) > 12 else False
                if is_favorited:
                    # å¢åŠ  0.2 çš„ç›¸ä¼¼åº¦æƒé‡ï¼Œç¡®ä¿æ”¶è—æ–‡ç« æ’åœ¨å‰é¢
                    similarity = min(1.0, similarity + 0.2)
                
                # å¤„ç† published_atï¼šå¯èƒ½æ˜¯ datetime å¯¹è±¡æˆ–å­—ç¬¦ä¸²
                published_at = row[8]
                if published_at:
                    if isinstance(published_at, datetime):
                        published_at_str = published_at.isoformat()
                    elif isinstance(published_at, str):
                        published_at_str = published_at
                    else:
                        published_at_str = str(published_at)
                else:
                    published_at_str = None
                
                # å¤„ç† tagsï¼šå¯èƒ½æ˜¯åˆ—è¡¨æˆ– JSON å­—ç¬¦ä¸²
                tags = row[10]
                if tags:
                    if isinstance(tags, str):
                        try:
                            tags = json.loads(tags)
                        except (json.JSONDecodeError, TypeError):
                            logger.warning(f"æ— æ³•è§£æ tags JSON: {tags}")
                            tags = []
                    elif not isinstance(tags, list):
                        tags = []
                else:
                    tags = []
                
                search_results.append({
                    "id": row[2],
                    "title": row[3],
                    "title_zh": row[4],
                    "url": row[5],
                    "summary": row[6],
                    "source": row[7],
                    "published_at": published_at_str,
                    "importance": row[9],
                    "tags": tags,
                    "similarity": similarity,
                    "is_favorited": is_favorited
                })
            
            # å»é‡ï¼šæŒ‰æ–‡ç« IDå»é‡ï¼Œä¿ç•™ç›¸ä¼¼åº¦æœ€é«˜çš„è®°å½•
            seen_article_ids = {}
            deduplicated_results = []
            for result in search_results:
                article_id = result["id"]
                if article_id not in seen_article_ids:
                    seen_article_ids[article_id] = result
                    deduplicated_results.append(result)
                else:
                    # å¦‚æœå·²å­˜åœ¨ï¼Œæ¯”è¾ƒç›¸ä¼¼åº¦ï¼Œä¿ç•™æ›´é«˜çš„
                    existing = seen_article_ids[article_id]
                    if result["similarity"] > existing["similarity"]:
                        # æ›¿æ¢ä¸ºç›¸ä¼¼åº¦æ›´é«˜çš„è®°å½•
                        index = deduplicated_results.index(existing)
                        deduplicated_results[index] = result
                        seen_article_ids[article_id] = result
            
            # æŒ‰ç›¸ä¼¼åº¦é‡æ–°æ’åºï¼ˆå»é‡åå¯èƒ½é¡ºåºæ”¹å˜ï¼‰
            deduplicated_results.sort(key=lambda x: x["similarity"], reverse=True)
            
            # é™åˆ¶è¿”å›æ•°é‡
            final_results = deduplicated_results[:top_k]
            
            logger.info(f"âœ… æœç´¢å®Œæˆï¼ˆä½¿ç”¨sqlite-vecï¼‰ï¼Œæ‰¾åˆ° {len(search_results)} ä¸ªç»“æœï¼Œå»é‡å {len(final_results)} ä¸ª")
            return final_results
            
        except Exception as e:
            logger.error(f"âŒ sqlite-vecæœç´¢å¤±è´¥: {e}ï¼Œå›é€€åˆ°Pythonè®¡ç®—")
            return self._search_with_python(query_embedding, top_k, filters)

    def _search_with_python(
        self,
        query_embedding: List[float],
        top_k: int,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """ä½¿ç”¨Pythonè¿›è¡Œå‘é‡æœç´¢ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        # è·å–æ‰€æœ‰å·²ç´¢å¼•çš„æ–‡ç« åµŒå…¥
        query_obj = self.db.query(ArticleEmbedding, Article).join(
            Article, ArticleEmbedding.article_id == Article.id
        )
        
        # åº”ç”¨è¿‡æ»¤æ¡ä»¶
        if filters:
            if filters.get("sources"):
                query_obj = query_obj.filter(Article.source.in_(filters["sources"]))
            
            if filters.get("importance"):
                query_obj = query_obj.filter(Article.importance.in_(filters["importance"]))
            
            if filters.get("time_from"):
                query_obj = query_obj.filter(Article.published_at >= filters["time_from"])
            
            if filters.get("time_to"):
                query_obj = query_obj.filter(Article.published_at <= filters["time_to"])
        
        # è·å–æ‰€æœ‰åŒ¹é…çš„æ–‡ç« åµŒå…¥
        embeddings = query_obj.all()
        
        if not embeddings:
            logger.warning("âš ï¸  æ²¡æœ‰æ‰¾åˆ°å·²ç´¢å¼•çš„æ–‡ç« ")
            return []
        
        # æ£€æŸ¥æŸ¥è¯¢å‘é‡ç»´åº¦
        query_dim = len(query_embedding)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        results = []
        for embedding_obj, article in embeddings:
            if not embedding_obj.embedding:
                continue
            
            stored_dim = len(embedding_obj.embedding)
            if query_dim != stored_dim:
                # è·³è¿‡ç»´åº¦ä¸åŒ¹é…çš„å‘é‡
                logger.debug(
                    f"âš ï¸  è·³è¿‡ç»´åº¦ä¸åŒ¹é…çš„æ–‡ç«  {article.id}ï¼š"
                    f"æŸ¥è¯¢å‘é‡ç»´åº¦ {query_dim}ï¼Œå­˜å‚¨å‘é‡ç»´åº¦ {stored_dim}"
                )
                continue
            
            similarity = self._cosine_similarity(query_embedding, embedding_obj.embedding)
            
            # å¦‚æœæ–‡ç« è¢«æ”¶è—ï¼Œå¢åŠ æƒé‡ï¼ˆæå‡ç›¸ä¼¼åº¦åˆ†æ•°ï¼‰
            if article.is_favorited:
                # å¢åŠ  0.2 çš„ç›¸ä¼¼åº¦æƒé‡ï¼Œç¡®ä¿æ”¶è—æ–‡ç« æ’åœ¨å‰é¢
                similarity = min(1.0, similarity + 0.2)
            
            results.append({
                "article": article,
                "similarity": similarity,
                "embedding_id": embedding_obj.id
            })
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        results.sort(key=lambda x: x["similarity"], reverse=True)
        
        # è¿”å›top_k
        top_results = results[:top_k]
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        search_results = []
        for result in top_results:
            article = result["article"]
            
            # å¤„ç† tagsï¼šç¡®ä¿æ˜¯åˆ—è¡¨
            tags = article.tags
            if tags and isinstance(tags, str):
                try:
                    tags = json.loads(tags)
                except (json.JSONDecodeError, TypeError):
                    logger.warning(f"æ— æ³•è§£æ tags JSON: {tags}")
                    tags = []
            elif not isinstance(tags, list):
                tags = tags if tags else []
            
            search_results.append({
                "id": article.id,
                "title": article.title,
                "title_zh": article.title_zh,
                "url": article.url,
                "summary": article.summary,
                "source": article.source,
                "published_at": article.published_at.isoformat() if article.published_at else None,
                "importance": article.importance,
                "tags": tags,
                "similarity": result["similarity"],
                "is_favorited": article.is_favorited
            })
        
        # å»é‡ï¼šæŒ‰æ–‡ç« IDå»é‡ï¼Œä¿ç•™ç›¸ä¼¼åº¦æœ€é«˜çš„è®°å½•
        seen_article_ids = {}
        deduplicated_results = []
        for result in search_results:
            article_id = result["id"]
            if article_id not in seen_article_ids:
                seen_article_ids[article_id] = result
                deduplicated_results.append(result)
            else:
                # å¦‚æœå·²å­˜åœ¨ï¼Œæ¯”è¾ƒç›¸ä¼¼åº¦ï¼Œä¿ç•™æ›´é«˜çš„
                existing = seen_article_ids[article_id]
                if result["similarity"] > existing["similarity"]:
                    # æ›¿æ¢ä¸ºç›¸ä¼¼åº¦æ›´é«˜çš„è®°å½•
                    index = deduplicated_results.index(existing)
                    deduplicated_results[index] = result
                    seen_article_ids[article_id] = result
        
        # æŒ‰ç›¸ä¼¼åº¦é‡æ–°æ’åºï¼ˆå»é‡åå¯èƒ½é¡ºåºæ”¹å˜ï¼‰
        deduplicated_results.sort(key=lambda x: x["similarity"], reverse=True)
        
        # é™åˆ¶è¿”å›æ•°é‡
        final_results = deduplicated_results[:top_k]
        
        logger.info(f"âœ… æœç´¢å®Œæˆï¼ˆä½¿ç”¨Pythonè®¡ç®—ï¼‰ï¼Œæ‰¾åˆ° {len(search_results)} ä¸ªç»“æœï¼Œå»é‡å {len(final_results)} ä¸ª")
        return final_results

    def query_articles(self, question: str, top_k: int = 5) -> Dict[str, Any]:
        """
        RAGé—®ç­”ï¼šåŸºäºæ£€ç´¢åˆ°çš„æ–‡ç« å›ç­”é—®é¢˜

        Args:
            question: é—®é¢˜æ–‡æœ¬
            top_k: æ£€ç´¢çš„æ–‡ç« æ•°é‡

        Returns:
            åŒ…å«ç­”æ¡ˆå’Œå¼•ç”¨æ–‡ç« çš„å­—å…¸
        """
        try:
            logger.info(f"ğŸ” å¼€å§‹é—®ç­”æµç¨‹: question={question[:100]}, top_k={top_k}")
            
            # æ£€ç´¢ç›¸å…³æ–‡ç« 
            try:
                relevant_articles = self.search_articles(question, top_k=top_k)
                logger.info(f"âœ… æ£€ç´¢åˆ° {len(relevant_articles)} ç¯‡ç›¸å…³æ–‡ç« ")
            except Exception as e:
                logger.error(f"âŒ æ£€ç´¢æ–‡ç« å¤±è´¥: {e}", exc_info=True)
                import traceback
                logger.error(f"æ£€ç´¢æ–‡ç« å®Œæ•´å †æ ˆ:\n{traceback.format_exc()}")
                raise
            
            if not relevant_articles:
                logger.warning("âš ï¸  æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡ç« ")
                return {
                    "answer": "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡ç« æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚",
                    "sources": [],
                    "articles": []
                }
            
            # æ„å»ºä¸Šä¸‹æ–‡
            try:
                context_parts = []
                for i, article_info in enumerate(relevant_articles, 1):
                    try:
                        article_text = f"""
æ–‡ç«  {i}:
æ ‡é¢˜: {article_info.get('title', 'N/A')}
"""
                        if article_info.get('title_zh'):
                            article_text += f"ä¸­æ–‡æ ‡é¢˜: {article_info['title_zh']}\n"
                        if article_info.get('summary'):
                            article_text += f"æ‘˜è¦: {article_info['summary']}\n"
                        article_text += f"æ¥æº: {article_info.get('source', 'N/A')}\n"
                        article_text += f"ç›¸ä¼¼åº¦: {article_info.get('similarity', 0):.3f}\n"
                        
                        context_parts.append(article_text)
                    except Exception as e:
                        logger.error(f"âŒ æ„å»ºæ–‡ç«  {i} ä¸Šä¸‹æ–‡å¤±è´¥: {e}", exc_info=True)
                        logger.error(f"æ–‡ç« ä¿¡æ¯: {article_info}")
                        continue
                
                context = "\n---\n".join(context_parts)
                logger.info(f"âœ… æ„å»ºä¸Šä¸‹æ–‡å®Œæˆï¼Œé•¿åº¦: {len(context)} å­—ç¬¦")
            except Exception as e:
                logger.error(f"âŒ æ„å»ºä¸Šä¸‹æ–‡å¤±è´¥: {e}", exc_info=True)
                import traceback
                logger.error(f"æ„å»ºä¸Šä¸‹æ–‡å®Œæ•´å †æ ˆ:\n{traceback.format_exc()}")
                raise
            
            # æ„å»ºæç¤ºè¯
            try:
                prompt = f"""åŸºäºä»¥ä¸‹æ–‡ç« å†…å®¹ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚è¯·ä½¿ç”¨ä¸­æ–‡å›ç­”ï¼Œå¹¶å¼•ç”¨å…·ä½“çš„æ–‡ç« ã€‚

ç›¸å…³æ–‡ç« ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·æä¾›è¯¦ç»†ã€å‡†ç¡®çš„ç­”æ¡ˆï¼Œå¹¶åœ¨å›ç­”ä¸­å¼•ç”¨ç›¸å…³çš„æ–‡ç« ã€‚å¼•ç”¨æ ¼å¼è¦æ±‚ï¼š
1. ä½¿ç”¨ [æ–‡ç« ç¼–å·] çš„æ ¼å¼å¼•ç”¨ï¼Œä¾‹å¦‚ï¼š[1] æåˆ°ï¼š"..." æˆ– [2] æŒ‡å‡ºï¼š...
2. ä¸è¦åœ¨å¼•ç”¨ä¸­åŒ…å«æ–‡ç« æ ‡é¢˜å’Œæ¥æºåç§°ï¼Œåªä½¿ç”¨ç¼–å·å¼•ç”¨
3. å¦‚æœæ–‡ç« ä¸­æ²¡æœ‰è¶³å¤Ÿçš„ä¿¡æ¯æ¥å›ç­”é—®é¢˜ï¼Œè¯·è¯´æ˜ã€‚"""
                logger.info(f"âœ… æç¤ºè¯æ„å»ºå®Œæˆï¼Œé•¿åº¦: {len(prompt)} å­—ç¬¦")
            except Exception as e:
                logger.error(f"âŒ æ„å»ºæç¤ºè¯å¤±è´¥: {e}", exc_info=True)
                raise
            
            # è°ƒç”¨LLMç”Ÿæˆç­”æ¡ˆ
            try:
                logger.info(f"ğŸ¤– æ­£åœ¨è°ƒç”¨LLMç”Ÿæˆç­”æ¡ˆ...")
                logger.debug(f"ä½¿ç”¨æ¨¡å‹: {self.ai_analyzer.model}")
                logger.debug(f"æç¤ºè¯å‰100å­—ç¬¦: {prompt[:100]}")
                
                response = self.ai_analyzer.client.chat.completions.create(
                    model=self.ai_analyzer.model,
                    messages=[
                        {
                            "role": "system",
                            "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIæ–°é—»åŠ©æ‰‹ï¼Œæ“…é•¿åŸºäºæä¾›çš„æ–‡ç« å†…å®¹å›ç­”é—®é¢˜ã€‚è¯·ä½¿ç”¨ä¸­æ–‡å›ç­”ï¼Œå¹¶å‡†ç¡®å¼•ç”¨æ–‡ç« æ¥æºã€‚"
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    temperature=0.3,
                    max_tokens=2000,
                )
                
                logger.info(f"âœ… LLMå“åº”æ¥æ”¶æˆåŠŸ")
                logger.debug(f"å“åº”å¯¹è±¡ç±»å‹: {type(response)}")
                logger.debug(f"å“åº”choicesæ•°é‡: {len(response.choices) if hasattr(response, 'choices') else 0}")
                
                if not response.choices:
                    raise ValueError("LLMå“åº”ä¸­æ²¡æœ‰choices")
                
                answer = response.choices[0].message.content.strip()
                logger.info(f"âœ… ç­”æ¡ˆç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(answer)} å­—ç¬¦")
                
            except Exception as e:
                logger.error(f"âŒ è°ƒç”¨LLMå¤±è´¥: {e}", exc_info=True)
                logger.error(f"LLMå®¢æˆ·ç«¯ç±»å‹: {type(self.ai_analyzer.client)}")
                logger.error(f"æ¨¡å‹åç§°: {self.ai_analyzer.model}")
                import traceback
                logger.error(f"LLMè°ƒç”¨å®Œæ•´å †æ ˆ:\n{traceback.format_exc()}")
                raise
            
            # æ„å»ºè¿”å›ç»“æœ
            try:
                sources = [article.get("source", "N/A") for article in relevant_articles]
                result = {
                    "answer": answer,
                    "sources": sources,
                    "articles": relevant_articles
                }
                logger.info(f"âœ… é—®ç­”æµç¨‹å®Œæˆ: answeré•¿åº¦={len(answer)}, sourcesæ•°é‡={len(sources)}, articlesæ•°é‡={len(relevant_articles)}")
                return result
            except Exception as e:
                logger.error(f"âŒ æ„å»ºè¿”å›ç»“æœå¤±è´¥: {e}", exc_info=True)
                import traceback
                logger.error(f"æ„å»ºè¿”å›ç»“æœå®Œæ•´å †æ ˆ:\n{traceback.format_exc()}")
                raise
            
        except Exception as e:
            logger.error(f"âŒ é—®ç­”å¤±è´¥: {e}", exc_info=True)
            import traceback
            logger.error(f"é—®ç­”å®Œæ•´å †æ ˆè·Ÿè¸ª:\n{traceback.format_exc()}")
            return {
                "answer": f"æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°é”™è¯¯: {str(e)}",
                "sources": [],
                "articles": []
            }

    def query_articles_stream(self, question: str, top_k: int = 5):
        """
        RAGé—®ç­”ï¼ˆæµå¼ï¼‰ï¼šåŸºäºæ£€ç´¢åˆ°çš„æ–‡ç« å›ç­”é—®é¢˜ï¼Œæ”¯æŒæµå¼è¾“å‡º

        Args:
            question: é—®é¢˜æ–‡æœ¬
            top_k: æ£€ç´¢çš„æ–‡ç« æ•°é‡

        Yields:
            æµå¼æ•°æ®å—ï¼ŒåŒ…å«ç±»å‹å’Œå†…å®¹
        """
        try:
            logger.info(f"ğŸ” å¼€å§‹æµå¼é—®ç­”æµç¨‹: question={question[:100]}, top_k={top_k}")
            
            # æ£€ç´¢ç›¸å…³æ–‡ç« 
            try:
                relevant_articles = self.search_articles(question, top_k=top_k)
                logger.info(f"âœ… æ£€ç´¢åˆ° {len(relevant_articles)} ç¯‡ç›¸å…³æ–‡ç« ")
                
                # å…ˆå‘é€æ–‡ç« ä¿¡æ¯
                yield {
                    "type": "articles",
                    "data": {
                        "articles": relevant_articles,
                        "sources": [article.get("source", "N/A") for article in relevant_articles]
                    }
                }
            except Exception as e:
                logger.error(f"âŒ æ£€ç´¢æ–‡ç« å¤±è´¥: {e}", exc_info=True)
                yield {
                    "type": "error",
                    "data": {"message": f"æ£€ç´¢æ–‡ç« å¤±è´¥: {str(e)}"}
                }
                return
            
            if not relevant_articles:
                logger.warning("âš ï¸  æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡ç« ")
                yield {
                    "type": "error",
                    "data": {"message": "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡ç« æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚"}
                }
                return
            
            # æ„å»ºä¸Šä¸‹æ–‡
            try:
                context_parts = []
                for i, article_info in enumerate(relevant_articles, 1):
                    try:
                        article_text = f"""
æ–‡ç«  {i}:
æ ‡é¢˜: {article_info.get('title', 'N/A')}
"""
                        if article_info.get('title_zh'):
                            article_text += f"ä¸­æ–‡æ ‡é¢˜: {article_info['title_zh']}\n"
                        if article_info.get('summary'):
                            article_text += f"æ‘˜è¦: {article_info['summary']}\n"
                        article_text += f"æ¥æº: {article_info.get('source', 'N/A')}\n"
                        article_text += f"ç›¸ä¼¼åº¦: {article_info.get('similarity', 0):.3f}\n"
                        
                        context_parts.append(article_text)
                    except Exception as e:
                        logger.error(f"âŒ æ„å»ºæ–‡ç«  {i} ä¸Šä¸‹æ–‡å¤±è´¥: {e}", exc_info=True)
                        logger.error(f"æ–‡ç« ä¿¡æ¯: {article_info}")
                        continue
                
                context = "\n---\n".join(context_parts)
                logger.info(f"âœ… æ„å»ºä¸Šä¸‹æ–‡å®Œæˆï¼Œé•¿åº¦: {len(context)} å­—ç¬¦")
            except Exception as e:
                logger.error(f"âŒ æ„å»ºä¸Šä¸‹æ–‡å¤±è´¥: {e}", exc_info=True)
                yield {
                    "type": "error",
                    "data": {"message": f"æ„å»ºä¸Šä¸‹æ–‡å¤±è´¥: {str(e)}"}
                }
                return
            
            # æ„å»ºæç¤ºè¯
            try:
                prompt = f"""åŸºäºä»¥ä¸‹æ–‡ç« å†…å®¹ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚è¯·ä½¿ç”¨ä¸­æ–‡å›ç­”ï¼Œå¹¶å¼•ç”¨å…·ä½“çš„æ–‡ç« ã€‚

ç›¸å…³æ–‡ç« ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·æä¾›è¯¦ç»†ã€å‡†ç¡®çš„ç­”æ¡ˆï¼Œå¹¶åœ¨å›ç­”ä¸­å¼•ç”¨ç›¸å…³çš„æ–‡ç« ã€‚å¼•ç”¨æ ¼å¼è¦æ±‚ï¼š
1. ä½¿ç”¨ [æ–‡ç« ç¼–å·] çš„æ ¼å¼å¼•ç”¨ï¼Œä¾‹å¦‚ï¼š[1] æåˆ°ï¼š"..." æˆ– [2] æŒ‡å‡ºï¼š...
2. ä¸è¦åœ¨å¼•ç”¨ä¸­åŒ…å«æ–‡ç« æ ‡é¢˜å’Œæ¥æºåç§°ï¼Œåªä½¿ç”¨ç¼–å·å¼•ç”¨
3. å¦‚æœæ–‡ç« ä¸­æ²¡æœ‰è¶³å¤Ÿçš„ä¿¡æ¯æ¥å›ç­”é—®é¢˜ï¼Œè¯·è¯´æ˜ã€‚"""
                logger.info(f"âœ… æç¤ºè¯æ„å»ºå®Œæˆï¼Œé•¿åº¦: {len(prompt)} å­—ç¬¦")
            except Exception as e:
                logger.error(f"âŒ æ„å»ºæç¤ºè¯å¤±è´¥: {e}", exc_info=True)
                yield {
                    "type": "error",
                    "data": {"message": f"æ„å»ºæç¤ºè¯å¤±è´¥: {str(e)}"}
                }
                return
            
            # è°ƒç”¨LLMç”Ÿæˆç­”æ¡ˆï¼ˆæµå¼ï¼‰
            try:
                logger.info(f"ğŸ¤– æ­£åœ¨è°ƒç”¨LLMç”Ÿæˆç­”æ¡ˆï¼ˆæµå¼ï¼‰...")
                logger.debug(f"ä½¿ç”¨æ¨¡å‹: {self.ai_analyzer.model}")
                
                stream = self.ai_analyzer.client.chat.completions.create(
                    model=self.ai_analyzer.model,
                    messages=[
                        {
                            "role": "system",
                            "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIæ–°é—»åŠ©æ‰‹ï¼Œæ“…é•¿åŸºäºæä¾›çš„æ–‡ç« å†…å®¹å›ç­”é—®é¢˜ã€‚è¯·ä½¿ç”¨ä¸­æ–‡å›ç­”ï¼Œå¹¶å‡†ç¡®å¼•ç”¨æ–‡ç« æ¥æºã€‚"
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    temperature=0.3,
                    max_tokens=2000,
                    stream=True,  # å¯ç”¨æµå¼è¾“å‡º
                )
                
                # æµå¼è¿”å›å†…å®¹
                for chunk in stream:
                    if chunk.choices and len(chunk.choices) > 0:
                        delta = chunk.choices[0].delta
                        if delta.content:
                            yield {
                                "type": "content",
                                "data": {"content": delta.content}
                            }
                
                # å‘é€å®Œæˆä¿¡å·
                yield {
                    "type": "done",
                    "data": {}
                }
                logger.info(f"âœ… æµå¼ç­”æ¡ˆç”Ÿæˆå®Œæˆ")
                
            except Exception as e:
                logger.error(f"âŒ è°ƒç”¨LLMå¤±è´¥: {e}", exc_info=True)
                yield {
                    "type": "error",
                    "data": {"message": f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°é”™è¯¯: {str(e)}"}
                }
                return
            
        except Exception as e:
            logger.error(f"âŒ æµå¼é—®ç­”å¤±è´¥: {e}", exc_info=True)
            import traceback
            logger.error(f"æµå¼é—®ç­”å®Œæ•´å †æ ˆè·Ÿè¸ª:\n{traceback.format_exc()}")
            yield {
                "type": "error",
                "data": {"message": f"æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°é”™è¯¯: {str(e)}"}
            }

    def get_index_stats(self) -> Dict[str, Any]:
        """
        è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            total_articles = self.db.query(Article).count()
            indexed_articles = self.db.query(ArticleEmbedding).count()
            unindexed_articles = total_articles - indexed_articles
            
            # æŒ‰æ¥æºç»Ÿè®¡
            source_stats = {}
            embeddings = self.db.query(ArticleEmbedding, Article).join(
                Article, ArticleEmbedding.article_id == Article.id
            ).all()
            
            for embedding_obj, article in embeddings:
                source = article.source
                if source not in source_stats:
                    source_stats[source] = 0
                source_stats[source] += 1
            
            return {
                "total_articles": total_articles,
                "indexed_articles": indexed_articles,
                "unindexed_articles": unindexed_articles,
                "index_coverage": indexed_articles / total_articles if total_articles > 0 else 0.0,
                "source_stats": source_stats
            }
        except Exception as e:
            logger.error(f"âŒ è·å–ç´¢å¼•ç»Ÿè®¡å¤±è´¥: {e}")
            return {
                "total_articles": 0,
                "indexed_articles": 0,
                "unindexed_articles": 0,
                "index_coverage": 0.0,
                "source_stats": {}
            }

